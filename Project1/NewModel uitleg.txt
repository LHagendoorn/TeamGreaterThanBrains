Oke, dus in het orginele HBOW-EM gebruiken ze gaussian mixture models met expectation maximasation om soft assignment te doen, waarbij ieder punt dus deel uit kan maken van meerdere clusters. Dit gebruiken ze voor zowel het clusteren (dus het constructen van de vocabulary) als voor de uiteindelijke indeling van de photo's per business aan de clusters, dus het maken van de histogram. Aangezien GMM alles in een keer in memory nodig heeft en enorme covariance matrices per cluster uitrekent, was dit compleet onpractisch voor een dataset met deze omvang. Daarom gebruiken we minibatch k-means, wat gewoon k-means is, maar dan gebruikt ie iedere keer maar een klein stuk in memory, en het werkt ook gewoon een stuk sneller. Dit geeft helaas dus wel geen optie om te soft assignen, en vandaar dat het constructen van de vocabulary gewoon gebeurt met k-means.
Het constructen van de histogram gebruikt ook soort van k-means. Maar het kijkt van alle datapunten wat de average distance is per cluster, en het gebruikt die afstand als selection criteria. Dus ieder punt dat dichterbij dan die afstand van een cluster ligt wordt bij die cluster geassigned. Dit is eigenlijk gewoon een willekeurige grens, maar ik moest wat. In ieder geval selecteerd ie zo dus meer dan 1 ding. Ik heb nog wel wat andere selectie dingen geprobeerd, en die leken minder goed te werken, maar volgensmij hadden we uiteindelijk niet echt tijd meer om die echt toe te passen en goed te testen.