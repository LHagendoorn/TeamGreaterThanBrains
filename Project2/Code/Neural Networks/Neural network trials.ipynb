{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Neural network \n",
    "=============\n",
    "\n",
    "This file contains lot's (and not even all) of the variations tried for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/media/sf_Project2/Code\"))\n",
    "from IO import Input\n",
    "from IO import Output\n",
    "import pandas as pd\n",
    "\n",
    "class cd:\n",
    "    \"\"\"Context manager for changing the current working directory\"\"\"\n",
    "    def __init__(self, newPath):\n",
    "        self.newPath = os.path.expanduser(newPath)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.savedPath = os.getcwd()\n",
    "        os.chdir(self.newPath)\n",
    "\n",
    "    def __exit__(self, etype, value, traceback):\n",
    "        os.chdir(self.savedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [],
   "source": [
    "with cd(\"/media/sf_Project2/Code\"): \n",
    "    train_dataset = np.array(Input.load_trainset_caffefeatures(featureSelectionMethod='RF',Percentile = 100)).astype('float32')\n",
    "    train_labels = np.array(Input.load_trainset_labels()).astype('float32')\n",
    "    valid_dataset = np.array(Input.load_validationset_caffefeatures(featureSelectionMethod='RF',Percentile = 100)).astype('float32')\n",
    "    valid_labels = np.array(Input.load_validationset_labels()).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with cd(\"/media/sf_Project2/Code\"): \n",
    "    #To be safe, do this in 3 parts from now on!!!\n",
    "    #test_data = np.array(Input.load_testdata_caffefeatures(True,range(38000),'RF',100)).astype('float32')\n",
    "    test_data = np.array(Input.load_testdata_caffefeatures(True,range(38000,80000),'RF',100)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41726, 3983)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Training set (14363, 3983) (14363, 10)\n",
      "Validation set (8061, 3983) (8061, 10)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "num_labels=10\n",
    "train_labels = np.squeeze((np.arange(num_labels) == train_labels[:,None]).astype(np.float32))\n",
    "valid_labels = np.squeeze((np.arange(num_labels) == valid_labels[:,None]).astype(np.float32))\n",
    "train_labels0 = train_labels[:,1]\n",
    "train_labels0 = train_labels0.reshape((train_labels.shape[0],1))\n",
    "\n",
    "print(train_labels0)\n",
    "#print(train_dataset)\n",
    "#train_labels = train_labels.reshape((train_labels.shape[0],1))\n",
    "#valid_labels = valid_labels.reshape((valid_labels.shape[0],1))\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "feature_size = train_dataset.shape[1]\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [],
   "source": [
    "#image_size = 28\n",
    "#num_labels = 10\n",
    "#num_channels = 1 # grayscale\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "#def reformat(dataset, labels):\n",
    "#  dataset = dataset.reshape(\n",
    "#    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "#  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "#  return dataset, labels\n",
    "#train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "#valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "#test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "#print('Training set', train_dataset.shape, train_labels.shape)\n",
    "#print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "#print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels,1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def singleAccuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(predictions == labels)\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "hlSize1 = 2048\n",
    "hlSize2 = 1024\n",
    "hlSize3 = 512\n",
    "hlSize4 = num_labels\n",
    "beta = 0.0001\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.int64, shape=(batch_size))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize1],\n",
    "            stddev=0.01))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize1]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize1,hlSize2], \n",
    "            stddev=0.01))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[hlSize2]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize2,hlSize3], \n",
    "            stddev=0.01))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.0, shape=[hlSize3]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize3,hlSize4], \n",
    "            stddev=0.01))\n",
    "  layer3_biases = tf.Variable(tf.constant(0.0, shape=[hlSize4]))\n",
    "  output_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize4,num_labels], \n",
    "            stddev=0.01))\n",
    "  output_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "    \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, layer1_weights) + layer1_biases)\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, layer2_weights) + layer2_biases)\n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3, layer3_weights) + layer3_biases)\n",
    "    layer5 = tf.matmul(layer4, output_weights) + output_biases\n",
    "    return layer4\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  #loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer2_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304760\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 100: 2.031671\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 200: 2.302646\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 300: 2.004498\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 400: 2.327179\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.3%\n",
      "Minibatch loss at step 500: 2.302585\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 600: 2.326915\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.5%\n",
      "Minibatch loss at step 700: 2.329764\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 800: 2.354721\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.5%\n",
      "Minibatch loss at step 900: 1.586996\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 1000: 2.372885\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "hlSize1 = 256\n",
    "beta = 0.0001\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize1],\n",
    "            stddev=0.01))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize1]))\n",
    "  output_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize1,num_labels], \n",
    "            stddev=0.01))\n",
    "  output_biases = tf.Variable(tf.constant(0.5, shape=[num_labels]))\n",
    "    \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.matmul(layer1, output_weights) + output_biases\n",
    "    return layer2\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "    beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.278514\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 100: 0.651811\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 200: 2.038844\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 300: 0.407706\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 400: 4.995739\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 500: 2.960555\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 600: 1.966929\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 12.0%\n",
      "Minibatch loss at step 700: 3.672642\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 800: 0.090602\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 900: 0.293407\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 1000: 3.027869\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 1100: 0.058071\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 1200: 1.643571\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 1300: 0.082644\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 1400: 1.248148\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 9.4%\n",
      "Minibatch loss at step 1500: 1.079450\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 1600: 0.150558\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 1700: 0.171246\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 8.6%\n",
      "Minibatch loss at step 1800: 0.087940\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 1900: 0.094974\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 2000: 0.082051\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 8.6%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(128, 10), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(128, 10), dtype=float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axis(=1) out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f511ec4f944f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_train_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m   \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf_train_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m   \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-28bfa86ad44c>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(predictions, labels)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels,1))\n\u001b[1;32m----> 3\u001b[1;33m           / predictions.shape[0])\n\u001b[0m",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[0margmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axis(=1) out of bounds"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_0 = 1024\n",
    "hidden_1 = 2048\n",
    "\n",
    "beta = 0.00001\n",
    "\n",
    "decay_steps = 100\n",
    "decay_rate = 0.90\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float64, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([feature_size, hidden_0], stddev=0.001))\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_0, hidden_1], stddev=0.001))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_1, num_labels], stddev=0.001))\n",
    "\n",
    "  biases_0 = tf.Variable(tf.zeros([hidden_0]))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_1]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Training computation.\n",
    "  learning_rate = tf.train.exponential_decay(0.3, global_step, decay_steps, decay_rate)\n",
    "  \n",
    "  logits_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_1 = tf.nn.relu(tf.matmul(logits_0, weights_1) + biases_1)\n",
    "  logits_2 = tf.matmul(logits_1, weights_2) + biases_2\n",
    "  \n",
    "  print(logits_2)\n",
    "  print(tf_train_labels)\n",
    "  loss = tf.reduce_mean(accuracy(logits_2,tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(weights_0) + beta * tf.nn.l2_loss(weights_1) + beta * tf.nn.l2_loss(weights_2)\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "      \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_2)\n",
    "\n",
    "  valid_0 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0)\n",
    "  valid_1 = tf.nn.relu(tf.matmul(valid_0, weights_1) + biases_1)\n",
    "  valid_2 = tf.matmul(valid_1, weights_2) + biases_2\n",
    "  valid_prediction = tf.nn.softmax(valid_2)\n",
    "\n",
    "    \n",
    "  #test_0 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0)\n",
    "  #test_1 = tf.nn.relu(tf.matmul(test_0, weights_1) + biases_1)\n",
    "  #test_2 = tf.matmul(test_1, weights_2) + biases_2\n",
    "  #test_prediction = tf.nn.softmax(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions, w0, w1, w2 = session.run(\n",
    "      [optimizer, loss, train_prediction, weights_0, weights_1, weights_2], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fresh attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 2024\n",
    "beta = 0.004\n",
    "#decay_steps = 200\n",
    "#decay_rate = 0.90\n",
    "#learningStart=0.0007\n",
    "decay_steps = 200\n",
    "decay_rate = 0.98\n",
    "learningStart=0.00007\n",
    "\n",
    "stdv = 0.03\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=stdv))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize0,num_labels], \n",
    "            stddev=stdv))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights) + layer1_biases\n",
    "    return layer2\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  #loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer1_weights)\n",
    "  #  beta * tf.nn.l2_loss(layer2_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(learningStart, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.612277\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 8.1%\n",
      "Minibatch loss at step 100: 3.081617\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.5%\n",
      "Minibatch loss at step 200: 2.154027\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 300: 3.649023\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 12.7%\n",
      "Minibatch loss at step 400: 2.254417\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 12.5%\n",
      "Minibatch loss at step 500: 2.471145\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 13.8%\n",
      "Minibatch loss at step 600: 2.232859\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 15.7%\n",
      "Minibatch loss at step 700: 1.418697\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 19.5%\n",
      "Minibatch loss at step 800: 1.038151\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 20.6%\n",
      "Minibatch loss at step 900: 1.474093\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 20.6%\n",
      "Minibatch loss at step 1000: 2.948368\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 22.5%\n",
      "Minibatch loss at step 1100: 1.326870\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 22.5%\n",
      "Minibatch loss at step 1200: 1.485084\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 24.4%\n",
      "Minibatch loss at step 1300: 1.359393\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 27.0%\n",
      "Minibatch loss at step 1400: 1.360483\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 26.8%\n",
      "Minibatch loss at step 1500: 1.273244\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 30.0%\n",
      "Minibatch loss at step 1600: 2.856008\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 28.9%\n",
      "Minibatch loss at step 1700: 1.706988\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 31.4%\n",
      "Minibatch loss at step 1800: 2.256593\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 27.9%\n",
      "Minibatch loss at step 1900: 1.145498\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 32.2%\n",
      "Minibatch loss at step 2000: 1.211704\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 32.3%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")\n",
    "  input_weights_val = input_weights.eval()\n",
    "  input_biases_val = input_biases.eval()\n",
    "  layer1_weights_val = layer1_weights.eval()\n",
    "  layer1_biases_val = layer1_biases.eval()\n",
    "  valid_prediction_val = valid_prediction.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validData = pd.DataFrame(valid_prediction_val)\n",
    "Output.to_outputfile(validData,1,'NNfinalishValid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12555464  0.02739759  0.10776729 ...,  0.04831534  0.0630124\n",
      "   0.2595064 ]\n",
      " [ 0.29324695  0.04345983  0.0160896  ...,  0.00503101  0.03180047\n",
      "   0.14383648]\n",
      " [ 0.29037118  0.03378007  0.26029718 ...,  0.10769741  0.14575382\n",
      "   0.02514734]\n",
      " ..., \n",
      " [ 0.05630442  0.02067383  0.02101005 ...,  0.00987583  0.0020766\n",
      "   0.01402664]\n",
      " [ 0.01897218  0.03959429  0.01928712 ...,  0.00068358  0.02324794\n",
      "   0.08103255]\n",
      " [ 0.34150296  0.01651141  0.00100533 ...,  0.00097191  0.01560758\n",
      "   0.32296211]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    firstHalfTest = tf.nn.softmax(layer2).eval()\n",
    "print(firstHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22806558  0.12997095  0.13495874 ...,  0.08459295  0.04555381\n",
      "   0.0557115 ]\n",
      " [ 0.09180526  0.03840186  0.03776614 ...,  0.08799823  0.07087118\n",
      "   0.47037387]\n",
      " [ 0.0542636   0.04385307  0.05653396 ...,  0.00983469  0.03361567\n",
      "   0.24700759]\n",
      " ..., \n",
      " [ 0.02237973  0.00690734  0.00548339 ...,  0.01811261  0.03638378\n",
      "   0.41283646]\n",
      " [ 0.072421    0.04396648  0.02306206 ...,  0.29994926  0.19569038\n",
      "   0.00929809]\n",
      " [ 0.12580341  0.01225203  0.00352421 ...,  0.01479877  0.5426181\n",
      "   0.06715485]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    thirdHalfTest = tf.nn.softmax(layer2).eval()\n",
    "print(thirdHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.69661488e-02   3.39022502e-02   3.09263589e-03 ...,   2.77760089e-03\n",
      "    1.14528432e-01   2.57804483e-01]\n",
      " [  1.83426782e-01   1.73101678e-01   3.62384878e-02 ...,   7.64091983e-02\n",
      "    1.21946298e-01   1.09924920e-01]\n",
      " [  4.56958413e-02   2.12017186e-02   2.82088757e-01 ...,   3.06101225e-04\n",
      "    1.67002767e-01   7.23533556e-02]\n",
      " ..., \n",
      " [  3.07071675e-02   5.74692339e-02   6.35425821e-02 ...,   1.23854040e-03\n",
      "    6.91337138e-02   1.00241728e-01]\n",
      " [  3.04357093e-02   1.16679333e-02   1.41384572e-01 ...,   3.32707077e-01\n",
      "    2.70471632e-01   4.45370376e-02]\n",
      " [  6.08687289e-02   3.28430422e-02   3.75269470e-03 ...,   6.51790434e-03\n",
      "    8.28824416e-02   1.94906190e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    secondHalfTest = tf.nn.softmax(layer2).eval()\n",
    "print(secondHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testClass = np.concatenate([firstHalfTest,secondHalfTest],0)\n",
    "testClass = pd.DataFrame(testClass)\n",
    "Output.to_outputfile(testClass,2,\"NNfinalish\",validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only classifying the first label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 128\n",
    "beta = 0.0001\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=0.01))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize0,num_labels], \n",
    "            stddev=0.01))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "  output_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_labels,1],\n",
    "            stddev=0.01))\n",
    "  output_biases = 0\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, layer1_weights) + layer1_biases)\n",
    "    layer3 = tf.matmul(layer2, output_weights) + output_biases\n",
    "    return layer3\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  #loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer2_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.692604\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 100: 0.663300\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 200: 0.103247\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 300: 2.119647\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 400: 0.073297\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 500: 0.073943\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 600: 0.091196\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 700: 0.183572\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 800: 0.174383\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 900: 0.121723\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "Minibatch loss at step 1000: 0.057694\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 20.4%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels0.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels0[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % singleAccuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % singleAccuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a05a8f3759b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/tmp/model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 2024\n",
    "hlSize1 = 1024\n",
    "beta = 0.05\n",
    "decay_steps = 200\n",
    "decay_rate = 0.90\n",
    "learningStart=0.0007\n",
    "stdv = 0.003\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=stdv))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize0,hlSize1], \n",
    "            stddev=stdv))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[hlSize1]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize1,num_labels], \n",
    "            stddev=stdv))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "    \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, layer1_weights) + layer1_biases)\n",
    "    layer3 = tf.matmul(layer2, layer2_weights) + layer2_biases\n",
    "    return layer3\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer2_weights)\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(learningStart, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.067645\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 12.0%\n",
      "Minibatch loss at step 100: 4.055054\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 13.2%\n",
      "Minibatch loss at step 200: 4.048203\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 13.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-29286f33bb6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     _, l, predictions = session.run(\n\u001b[1;32m---> 12\u001b[1;33m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")\n",
    "  input_weights_val = input_weights.eval()\n",
    "  input_biases_val = input_biases.eval()\n",
    "  layer1_weights_val = layer1_weights.eval()\n",
    "  layer1_biases_val = layer1_biases.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1x1 convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (64, 2024) must have rank 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-950e66578d8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \u001b[1;31m# Training computation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m   \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m   \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-950e66578d8e>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#layer1 = tf.reshape(layer1,[hlSize0,1,1,1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer1_biases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mlayer3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2_weights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer2_biases\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, name)\u001b[0m\n\u001b[0;32m    209\u001b[0m   return _op_def_lib.apply_op(\"Conv2D\", input=input, filter=filter,\n\u001b[0;32m    210\u001b[0m                               \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                               use_cudnn_on_gpu=use_cudnn_on_gpu, name=name)\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    653\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    654\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    656\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Restructure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2041\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2043\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1526\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1527\u001b[0m                          % op.type)\n\u001b[1;32m-> 1528\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1529\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/common_shapes.pyc\u001b[0m in \u001b[0;36mconv2d_shape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mare\u001b[0m \u001b[0mincompatible\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m   \"\"\"\n\u001b[1;32m--> 176\u001b[1;33m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m   \u001b[0mfilter_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aiws/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mwith_rank\u001b[1;34m(self, rank)\u001b[0m\n\u001b[0;32m    614\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknown_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape %s must have rank %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwith_rank_at_least\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape (64, 2024) must have rank 4"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 2024\n",
    "beta = 0.01\n",
    "decay_steps = 200\n",
    "decay_rate = 0.90\n",
    "learningStart=0.0007\n",
    "stdv = 0.03\n",
    "patch_size = 1\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=stdv))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, num_labels], \n",
    "            stddev=stdv))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_labels,num_labels], \n",
    "            stddev=stdv))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    print(tf.shape(layer1))\n",
    "    #layer1 = tf.reshape(layer1,[hlSize0,1,1,1])\n",
    "    conv = tf.nn.conv2d(layer1, layer1_weights, [1, 1], padding='SAME')\n",
    "    layer2 = tf.nn.relu(conv + layer1_biases)\n",
    "    layer3 = tf.matmul(layer2, layer2_weights) + layer2_biases\n",
    "    return layer3\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer2_weights)\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(learningStart, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 36.045696\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.7%\n",
      "Minibatch loss at step 100: 36.571320\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 16.9%\n",
      "Minibatch loss at step 200: 31.054327\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 25.9%\n",
      "Minibatch loss at step 300: 28.318371\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 25.8%\n",
      "Minibatch loss at step 400: 29.402561\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 22.4%\n",
      "Minibatch loss at step 500: 29.327303\n",
      "Minibatch accuracy: 32.8%\n",
      "Validation accuracy: 29.0%\n",
      "Minibatch loss at step 600: 30.051636\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 33.8%\n",
      "Minibatch loss at step 700: 28.164223\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 30.9%\n",
      "Minibatch loss at step 800: 29.349073\n",
      "Minibatch accuracy: 35.9%\n",
      "Validation accuracy: 37.3%\n",
      "Minibatch loss at step 900: 28.201241\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 42.9%\n",
      "Minibatch loss at step 1000: 31.330561\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 39.0%\n",
      "Minibatch loss at step 1100: 28.205492\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 41.4%\n",
      "Minibatch loss at step 1200: 27.930679\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 40.5%\n",
      "Minibatch loss at step 1300: 28.705837\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 44.9%\n",
      "Minibatch loss at step 1400: 28.098713\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 35.1%\n",
      "Minibatch loss at step 1500: 28.053869\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 44.2%\n",
      "Minibatch loss at step 1600: 29.741819\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 44.8%\n",
      "Minibatch loss at step 1700: 28.858957\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 47.5%\n",
      "Minibatch loss at step 1800: 28.510069\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 46.1%\n",
      "Minibatch loss at step 1900: 28.545172\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 2000: 27.989004\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 45.4%\n",
      "Minibatch loss at step 2100: 28.451973\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 46.9%\n",
      "Minibatch loss at step 2200: 27.955606\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 2300: 27.752499\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 45.2%\n",
      "Minibatch loss at step 2400: 27.820448\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 47.4%\n",
      "Minibatch loss at step 2500: 27.759480\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 47.2%\n",
      "Minibatch loss at step 2600: 28.026991\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 48.4%\n",
      "Minibatch loss at step 2700: 27.699310\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 48.6%\n",
      "Minibatch loss at step 2800: 27.654074\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 47.5%\n",
      "Minibatch loss at step 2900: 27.946457\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 47.0%\n",
      "Minibatch loss at step 3000: 27.800785\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 48.9%\n",
      "Minibatch loss at step 3100: 27.716450\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 49.3%\n",
      "Minibatch loss at step 3200: 27.813841\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 49.1%\n",
      "Minibatch loss at step 3300: 28.346138\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3400: 27.972630\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 49.3%\n",
      "Minibatch loss at step 3500: 27.643139\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 3600: 27.831541\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 48.7%\n",
      "Minibatch loss at step 3700: 28.017679\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3800: 27.830009\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3900: 27.752213\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 28.378363\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 50.0%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")\n",
    "  input_weights_val = input_weights.eval()\n",
    "  input_biases_val = input_biases.eval()\n",
    "  layer1_weights_val = layer1_weights.eval()\n",
    "  layer1_biases_val = layer1_biases.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 2024\n",
    "hlSize1 = 1024\n",
    "beta = 0.05\n",
    "decay_steps = 100\n",
    "decay_rate = 0.90\n",
    "learningStart=0.0007\n",
    "#learning_rate=0.001\n",
    "stdv = 0.003\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=stdv))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize0,hlSize1], \n",
    "            stddev=stdv))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[hlSize1]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize1,num_labels], \n",
    "            stddev=stdv))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "    \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, layer1_weights) + layer1_biases)\n",
    "    layer3 = tf.matmul(layer2, layer2_weights) + layer2_biases\n",
    "    return layer3\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer2_weights)\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(learningStart, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.016086\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 100: 2.970623\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 200: 2.955587\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 300: 2.891662\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 400: 2.883225\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 500: 2.874598\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 600: 2.810019\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 700: 2.796701\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 800: 2.777732\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")\n",
    "  input_weights_val = input_weights.eval()\n",
    "  input_biases_val = input_biases.eval()\n",
    "  layer1_weights_val = layer1_weights.eval()\n",
    "  layer1_biases_val = layer1_biases.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00078307  0.00095813  0.00080282 ...,  0.0007355   0.00077737\n",
      "   0.00073058]\n",
      " [ 0.00093672  0.00094288  0.00091188 ...,  0.00072089  0.00076084\n",
      "   0.00071527]\n",
      " [ 0.00058513  0.00088732  0.00061786 ...,  0.0007136   0.00073604\n",
      "   0.00071236]\n",
      " ..., \n",
      " [ 0.00089966  0.00088711  0.0008768  ...,  0.00066718  0.00072292\n",
      "   0.00066195]\n",
      " [ 0.00075745  0.00110351  0.00081773 ...,  0.00074436  0.00075917\n",
      "   0.00074965]\n",
      " [ 0.00069846  0.00069995  0.00067106 ...,  0.00062636  0.00067104\n",
      "   0.00062526]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    firstHalfTest = tf.nn.softmax(layer2).eval()\n",
    "print(firstHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00111596  0.00091727  0.00106503 ...,  0.0006479   0.00073936\n",
      "   0.00065319]\n",
      " [ 0.00090069  0.00105173  0.00092015 ...,  0.00072383  0.00077124\n",
      "   0.0007149 ]\n",
      " [ 0.00084777  0.00083204  0.0008227  ...,  0.00066847  0.00072262\n",
      "   0.00066509]\n",
      " ..., \n",
      " [ 0.00105296  0.00102638  0.00105567 ...,  0.00069359  0.00075461\n",
      "   0.00069679]\n",
      " [ 0.0007897   0.00106825  0.00083355 ...,  0.00063956  0.00067962\n",
      "   0.0006287 ]\n",
      " [ 0.0007754   0.00076975  0.00073611 ...,  0.00071936  0.00077459\n",
      "   0.00071817]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    secondHalfTest = tf.nn.softmax(layer2).eval()\n",
    "print(secondHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c51a61d03a20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtestClass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirstHalfTest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msecondHalfTest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtestClass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestClass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_outputfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestClass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"NNtest3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "testClass = np.concatenate([firstHalfTest,secondHalfTest],0)\n",
    "testClass = pd.DataFrame(testClass)\n",
    "Output.to_outputfile(testClass,2,\"NNtest3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 2000\n",
    "hlSize1 = 1000\n",
    "beta = 0.001\n",
    "#decay_steps = 200\n",
    "#decay_rate = 0.90\n",
    "#learningStart=0.0007\n",
    "decay_steps = 200\n",
    "decay_rate = 0.90\n",
    "learningStart=0.0007\n",
    "keep_prob = 0.5\n",
    "\n",
    "stdv = 0.03\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=stdv))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize0,hlSize1], \n",
    "            stddev=stdv))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[hlSize1]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize1,num_labels], \n",
    "            stddev=stdv))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data,train):\n",
    "    if (train == 1):\n",
    "        layer1 = tf.nn.relu(tf.matmul(tf.nn.dropout(data, keep_prob), input_weights) + input_biases)\n",
    "    else :\n",
    "        layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights) + layer1_biases\n",
    "    layer3 = tf.matmul(layer2, layer2_weights) + layer2_biases\n",
    "    return layer3\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset,1)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer2_weights)\n",
    "  #  beta * tf.nn.l2_loss(layer3_weights) + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  #learning_rate = tf.train.exponential_decay(learningStart, global_step, decay_steps, decay_rate)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  #optimizer = tf.train.AdadeltaOptimizer().minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset,0))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 7.311119\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 100: 8.242072\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 14.9%\n",
      "Minibatch loss at step 200: 5.363883\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 28.2%\n",
      "Minibatch loss at step 300: 4.963015\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 17.8%\n",
      "Minibatch loss at step 400: 4.990660\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 500: 4.696717\n",
      "Minibatch accuracy: 29.7%\n",
      "Validation accuracy: 32.4%\n",
      "Minibatch loss at step 600: 5.082463\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 34.8%\n",
      "Minibatch loss at step 700: 4.118400\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 800: 5.142662\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 32.3%\n",
      "Minibatch loss at step 900: 3.764737\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 35.4%\n",
      "Minibatch loss at step 1000: 6.442114\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 39.6%\n",
      "Minibatch loss at step 1100: 4.192168\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 35.7%\n",
      "Minibatch loss at step 1200: 3.555025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 33.7%\n",
      "Minibatch loss at step 1300: 4.326628\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 38.9%\n",
      "Minibatch loss at step 1400: 3.586116\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 28.3%\n",
      "Minibatch loss at step 1500: 3.824551\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 39.4%\n",
      "Minibatch loss at step 1600: 5.460207\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 38.7%\n",
      "Minibatch loss at step 1700: 4.756928\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 41.6%\n",
      "Minibatch loss at step 1800: 4.573057\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 38.1%\n",
      "Minibatch loss at step 1900: 4.358685\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 41.4%\n",
      "Minibatch loss at step 2000: 4.058342\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 41.9%\n",
      "Minibatch loss at step 2100: 4.356015\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 40.3%\n",
      "Minibatch loss at step 2200: 3.739482\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 42.4%\n",
      "Minibatch loss at step 2300: 3.302911\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 36.4%\n",
      "Minibatch loss at step 2400: 3.725439\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 44.7%\n",
      "Minibatch loss at step 2500: 3.502416\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 36.9%\n",
      "Minibatch loss at step 2600: 4.075659\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 41.8%\n",
      "Minibatch loss at step 2700: 3.339075\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 42.7%\n",
      "Minibatch loss at step 2800: 3.443807\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 43.5%\n",
      "Minibatch loss at step 2900: 3.879068\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 44.9%\n",
      "Minibatch loss at step 3000: 3.328058\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 41.6%\n",
      "Minibatch loss at step 3100: 3.510762\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 45.8%\n",
      "Minibatch loss at step 3200: 3.855144\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 43.1%\n",
      "Minibatch loss at step 3300: 4.323146\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 3400: 3.635057\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 43.0%\n",
      "Minibatch loss at step 3500: 3.260158\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 46.1%\n",
      "Minibatch loss at step 3600: 3.531471\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 44.4%\n",
      "Minibatch loss at step 3700: 4.059417\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 47.3%\n",
      "Minibatch loss at step 3800: 3.894932\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 45.4%\n",
      "Minibatch loss at step 3900: 3.385466\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 44.2%\n",
      "Minibatch loss at step 4000: 4.541009\n",
      "Minibatch accuracy: 42.2%\n",
      "Validation accuracy: 45.0%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")\n",
    "  input_weights_val = input_weights.eval()\n",
    "  input_biases_val = input_biases.eval()\n",
    "  layer1_weights_val = layer1_weights.eval()\n",
    "  layer1_biases_val = layer1_biases.eval()\n",
    "  layer2_biases_val = layer2_biases.eval()\n",
    "  layer2_weights_val = layer2_weights.eval()\n",
    "  valid_prediction_val = valid_prediction.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with tf.Session(graph=graph) as session:\n",
    "#    layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, input_weights) + input_biases)\n",
    "#    layer2 = tf.matmul(layer1, layer1_weights) + layer1_biases\n",
    "#    layer3 = tf.nn.softmax(tf.matmul(layer2, layer2_weights) + layer2_biases)\n",
    "    #layer1 = tf.nn.relu(tf.matmul(valid_dataset, input_weights_val) + input_biases_val)\n",
    "    #layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "#    validData = layer3.eval()\n",
    "\n",
    "validData = pd.DataFrame(valid_prediction_val)\n",
    "validData.to_csv('NN4validPredictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.84411931e-03   7.60621503e-02   2.29734927e-01 ...,   4.21042517e-02\n",
      "    1.00717030e-01   1.78937957e-01]\n",
      " [  1.42821223e-01   1.02126941e-01   1.22120162e-03 ...,   2.20743148e-03\n",
      "    4.38187905e-02   2.06218302e-01]\n",
      " [  4.55989063e-01   2.79622585e-01   4.11956757e-02 ...,   1.27872750e-02\n",
      "    4.97623086e-02   1.54123932e-01]\n",
      " ..., \n",
      " [  9.94103961e-03   7.82047585e-02   1.27162505e-02 ...,   3.07481340e-03\n",
      "    1.91718638e-01   8.68645608e-02]\n",
      " [  8.97621829e-03   1.87869256e-04   6.29380907e-07 ...,   3.62184946e-04\n",
      "    6.55896366e-02   8.98753777e-02]\n",
      " [  1.61460806e-02   5.18567741e-01   2.87237763e-03 ...,   1.07305276e-03\n",
      "    2.03112084e-02   2.16739904e-02]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    layer3 = tf.matmul(layer2, layer2_weights_val) + layer2_biases_val\n",
    "    firstHalfTest = tf.nn.softmax(layer3).eval()\n",
    "print(firstHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    layer3 = tf.matmul(layer2, layer2_weights_val) + layer2_biases_val\n",
    "    firstHalfTest = np.concatenate([firstHalfTest,tf.nn.softmax(layer3).eval()],0)\n",
    "#print(secondHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    layer3 = tf.matmul(layer2, layer2_weights_val) + layer2_biases_val\n",
    "    firstHalfTest = np.concatenate([firstHalfTest,tf.nn.softmax(layer3).eval()],0)\n",
    "#print(thirdHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    layer3 = tf.matmul(layer2, layer2_weights_val) + layer2_biases_val\n",
    "    firstHalfTest = np.concatenate([firstHalfTest,tf.nn.softmax(layer3).eval()],0)\n",
    "#print(fourthHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testClass = np.concatenate([firstHalfTest,secondHalfTest,thirdHalfTest,fourthHalfTest],0)\n",
    "testClass = pd.DataFrame(firstHalfTest)\n",
    "Output.to_outputfile(testClass,2,\"NNtest4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## moar layerzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hlSize0 = 2000\n",
    "hlSize1 = 1000\n",
    "hlSize2 = 500\n",
    "beta = 0.001\n",
    "#decay_steps = 200\n",
    "#decay_rate = 0.90\n",
    "#learningStart=0.0007\n",
    "decay_steps = 200\n",
    "decay_rate = 0.90\n",
    "learningStart=0.0007\n",
    "keep_prob = 0.5\n",
    "\n",
    "stdv = 0.03\n",
    "#patch_size = 5\n",
    "#depth = 16\n",
    "#num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  input_weights = tf.Variable(tf.truncated_normal(\n",
    "      [feature_size,hlSize0],\n",
    "            stddev=stdv))\n",
    "  input_biases = tf.Variable(tf.zeros([hlSize0]))\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize0,hlSize1], \n",
    "            stddev=stdv))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.0, shape=[hlSize1]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize1,hlSize2], \n",
    "            stddev=stdv))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.0, shape=[hlSize2]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [hlSize2,num_labels], \n",
    "            stddev=stdv))\n",
    "  layer3_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    "  \n",
    "  \n",
    "  # Model.\n",
    "  def model(data,train):\n",
    "    if (train == 1):\n",
    "        layer1 = tf.nn.relu(tf.matmul(tf.nn.dropout(data, keep_prob), input_weights) + input_biases)\n",
    "    else :\n",
    "        layer1 = tf.nn.relu(tf.matmul(data, input_weights) + input_biases)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights) + layer1_biases\n",
    "    layer3 = tf.matmul(layer2, layer2_weights) + layer2_biases\n",
    "    layer4 = tf.matmul(layer3, layer3_weights) + layer3_biases\n",
    "    return layer4\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset,1)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta * tf.nn.l2_loss(input_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer1_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer2_weights) + \\\n",
    "    beta * tf.nn.l2_loss(layer3_weights)# + \\\n",
    "  #  beta * tf.nn.l2_loss(output_weights)\n",
    "\n",
    "  # Optimizer.\n",
    "  #learning_rate = tf.train.exponential_decay(learningStart, global_step, decay_steps, decay_rate)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  #optimizer = tf.train.AdadeltaOptimizer().minimize(loss)\n",
    "  optimizer = tf.train.AdagradOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset,0))\n",
    "  #test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6.297700\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 100: 6.483749\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 17.3%\n",
      "Minibatch loss at step 200: 5.459052\n",
      "Minibatch accuracy: 20.3%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 300: 4.921887\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 20.9%\n",
      "Minibatch loss at step 400: 5.052115\n",
      "Minibatch accuracy: 29.7%\n",
      "Validation accuracy: 29.6%\n",
      "Minibatch loss at step 500: 5.185280\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 29.8%\n",
      "Minibatch loss at step 600: 5.098585\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 35.1%\n",
      "Minibatch loss at step 700: 4.716326\n",
      "Minibatch accuracy: 35.9%\n",
      "Validation accuracy: 28.8%\n",
      "Minibatch loss at step 800: 5.438993\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 36.2%\n",
      "Minibatch loss at step 900: 4.208798\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 37.2%\n",
      "Minibatch loss at step 1000: 6.145782\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 41.2%\n",
      "Minibatch loss at step 1100: 4.131027\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 39.0%\n",
      "Minibatch loss at step 1200: 4.132130\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 34.0%\n",
      "Minibatch loss at step 1300: 4.645090\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 42.1%\n",
      "Minibatch loss at step 1400: 3.857773\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 30.8%\n",
      "Minibatch loss at step 1500: 4.160034\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 41.7%\n",
      "Minibatch loss at step 1600: 5.526233\n",
      "Minibatch accuracy: 35.9%\n",
      "Validation accuracy: 38.0%\n",
      "Minibatch loss at step 1700: 4.571145\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 43.2%\n",
      "Minibatch loss at step 1800: 4.960834\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 39.6%\n",
      "Minibatch loss at step 1900: 4.512097\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 43.4%\n",
      "Minibatch loss at step 2000: 4.154681\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 43.7%\n",
      "Minibatch loss at step 2100: 4.341901\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 41.0%\n",
      "Minibatch loss at step 2200: 3.999470\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 43.8%\n",
      "Minibatch loss at step 2300: 3.639815\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 40.1%\n",
      "Minibatch loss at step 2400: 4.078984\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 46.3%\n",
      "Minibatch loss at step 2500: 3.647178\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 36.9%\n",
      "Minibatch loss at step 2600: 4.502290\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 45.5%\n",
      "Minibatch loss at step 2700: 3.684946\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 44.5%\n",
      "Minibatch loss at step 2800: 3.720910\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 46.5%\n",
      "Minibatch loss at step 2900: 4.097938\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 46.5%\n",
      "Minibatch loss at step 3000: 3.440453\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 3100: 3.717615\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 45.9%\n",
      "Minibatch loss at step 3200: 4.118452\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 44.2%\n",
      "Minibatch loss at step 3300: 4.402783\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 46.3%\n",
      "Minibatch loss at step 3400: 3.835487\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 44.1%\n",
      "Minibatch loss at step 3500: 3.441394\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 47.3%\n",
      "Minibatch loss at step 3600: 3.750152\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 3700: 4.397746\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.2%\n",
      "Minibatch loss at step 3800: 4.190309\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 47.0%\n",
      "Minibatch loss at step 3900: 3.732838\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 47.4%\n",
      "Minibatch loss at step 4000: 5.097141\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 46.2%\n",
      "Minibatch loss at step 4100: 3.712650\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 42.0%\n",
      "Minibatch loss at step 4200: 4.163434\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4300: 3.451935\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 47.7%\n",
      "Minibatch loss at step 4400: 3.701244\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 47.4%\n",
      "Minibatch loss at step 4500: 3.463243\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 47.5%\n",
      "Minibatch loss at step 4600: 3.658668\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 47.9%\n",
      "Minibatch loss at step 4700: 3.814975\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 48.5%\n",
      "Minibatch loss at step 4800: 3.872131\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 47.8%\n",
      "Minibatch loss at step 4900: 4.196872\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 48.7%\n",
      "Minibatch loss at step 5000: 3.487808\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 46.3%\n",
      "Minibatch loss at step 5100: 4.106877\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 48.8%\n",
      "Minibatch loss at step 5200: 3.734140\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 44.6%\n",
      "Minibatch loss at step 5300: 4.068423\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 49.1%\n",
      "Minibatch loss at step 5400: 4.134588\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 5500: 4.140870\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5600: 3.617614\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 49.1%\n",
      "Minibatch loss at step 5700: 4.087208\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 5800: 3.592962\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 48.9%\n",
      "Minibatch loss at step 5900: 3.585838\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 6000: 3.569626\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 49.9%\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  print(\"finished!\")\n",
    "  input_weights_val = input_weights.eval()\n",
    "  input_biases_val = input_biases.eval()\n",
    "  layer1_weights_val = layer1_weights.eval()\n",
    "  layer1_biases_val = layer1_biases.eval()\n",
    "  layer2_weights_val = layer2_weights.eval()\n",
    "  layer2_biases_val = layer2_biases.eval()\n",
    "  layer3_weights_val = layer3_weights.eval()\n",
    "  layer3_biases_val = layer3_biases.eval()\n",
    "  valid_prediction_val = valid_prediction.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with tf.Session(graph=graph) as session:\n",
    "#    layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, input_weights) + input_biases)\n",
    "#    layer2 = tf.matmul(layer1, layer1_weights) + layer1_biases\n",
    "#    layer3 = tf.nn.softmax(tf.matmul(layer2, layer2_weights) + layer2_biases)\n",
    "    #layer1 = tf.nn.relu(tf.matmul(valid_dataset, input_weights_val) + input_biases_val)\n",
    "    #layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "#    validData = layer3.eval()\n",
    "\n",
    "validData = pd.DataFrame(valid_prediction_val)\n",
    "validData.to_csv('NN5validPredictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    layer3 = tf.matmul(layer2, layer2_weights_val) + layer2_biases_val\n",
    "    layer4 = tf.matmul(layer3, layer3_weights_val) + layer3_biases_val\n",
    "    testData = tf.nn.softmax(layer4).eval()\n",
    "#print(firstHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    layer1 = tf.nn.relu(tf.matmul(test_data, input_weights_val) + input_biases_val)\n",
    "    layer2 = tf.matmul(layer1, layer1_weights_val) + layer1_biases_val\n",
    "    layer3 = tf.matmul(layer2, layer2_weights_val) + layer2_biases_val\n",
    "    layer4 = tf.matmul(layer3, layer3_weights_val) + layer3_biases_val\n",
    "    testData = np.concatenate([testData,tf.nn.softmax(layer4).eval()],0)\n",
    "#print(firstHalfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testClass = np.concatenate([firstHalfTest,secondHalfTest,thirdHalfTest,fourthHalfTest],0)\n",
    "testClass = pd.DataFrame(testData)\n",
    "Output.to_outputfile(testClass,2,\"NNtest5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
